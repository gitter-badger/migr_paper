\documentclass{sig-alternate}
\usepackage{url}
\usepackage{pgffor}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{graphicx}% http://ctan.org/pkg/graphicx
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amssymb}
%\usepackage[]{algorithm2e}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{arrows, petri, topaths}
\usepackage{tkz-berge}

% Do-While
\algdef{SE}[DOWHILE]{Do}{DoWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%


\begin{document}

 \conferenceinfo{GECCO'15,} {July 11-15, 2015, Madrid, Spain.}
    \CopyrightYear{2015}
    \crdata{TBA}
    \clubpenalty=10000
    \widowpenalty = 10000

\title{A Study on Migration in the Heterogeneous Island Model}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}

\numberofauthors{4}
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
\alignauthor
Krzysztof Nowak\\
       \affaddr{Advanced Concepts Team}\\
       \affaddr{European Space Agency (ESTEC)}\\
       \affaddr{Noordwijk, The Netherlands}\\
       \email{Krzysztof.Nowak@esa.int}
\alignauthor
Dario Izzo\\
       \affaddr{Advanced Concepts Team}\\
       \affaddr{European Space Agency (ESTEC)}\\
       \affaddr{Noordwijk, The Netherlands}\\
       \email{Dario.Izzo@esa.int}
\alignauthor
Daniel Hennes\\
       \affaddr{Advanced Concepts Team}\\
       \affaddr{European Space Agency (ESTEC)}\\
       \affaddr{Noordwijk, The Netherlands}\\
       \email{Daniel.Hennes@esa.int}
}

\maketitle
\begin{abstract}
Metaheuristics have proven to be an efficient method of handling difficult global optimization tasks.
State of art algorithms utilize several optimization techniques, allowing for occasional information exchange to leverage the trade-off between the exploration and exploitation.
Besides the popularity of metaheuristics among the evolutionary strategies researchers, there is also an interest in exploring parallel models of computation.
In this paper we focus at both concepts and analyze the performance of the heterogeneous island models and the impact of the selected hyper-parameters on the optimization performance.
We propose a methodology for characterizing the migration interplay between three metaheuristics, and evaluate the performance on real--parameter single objective optimization benchmarks.
%We observe a high sensitivity to both the frequency of migration and the migration strategies.

%heterogeneous metaheuristics
\end{abstract}

% A category with the (minimum) three required fields
% TODO: Find category
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Global Optimization, Evolutionary Algorithms, Island Model, Parallel Computing, Parameter Tuning}

\keywords{Global Optimization, Evolutionary Algorithms, Island Model, Parallel Computing, Differential Evolution, Particle Swarm Optimization, CMA-ES}

\section{Introduction}
Search metaheuristics have been steadily growing in popularity over the recent years, mostly due to their performance in solving difficult optimization problems.
Although several search techniques are shown to perform well in global optimization, no single algorithm can claim to be universal~\cite{wolpert1997no}.
Popular algorithms, such as Differential Evolution (DE)~\cite{storn1997differential}, Particle Swarm Optimization (PSO)~\cite{james1995particle} or Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES)~\cite{hansen2001completely},~\cite{hansen2003reducing} are often used as the building blocks of more sophisticated strategies for leveraging the exploration--exploitation trade-off using hybridization~\cite{Liao:2013:CEC}, adaptiveness~\cite{qin2009differential} or multiple restarts~\cite{auger2005restart}.
% TODO: Find example for PSO for either of those

% TODO: CEC WINNERS
%memetic~\cite{Lacroix:2013:CEC}, hybridization~\cite{Liao:2013:CEC}, restart strategies~\cite{EPFL-CONF-191267}.

% TODO: not increasing popular, its an old concept
Parallel metaheuristics are increasingly popular, mainly due to the shift in the processor development industry towards the multiple-core architectures, as well as the higher availability and lowering costs of the distributed computing services and platforms.
Several approaches to parallelization were proposed in the past~\cite{cantu1998survey}.%TODO: Add more examples of parallelization
In this paper we focus on the coarse-grained, multi-population parallelism known as the island model~\cite{cantu1999topologies}.
With the high customization and flexibility of the island model come the increasing number of hyper--parameters, which are difficult due to the already complex model.
One of the increasingly popular approaches is using several different metaheuristics on each island, and allowing for interplay between them in the form of migration.
Although such heterogeneous setup in algorithm's parameters has been shown to be effective~\cite{jjmorelo2014pool},~\cite{gong2011}, the heterogeneity in terms of distinct metaheuristics has been only partially explored.
The benefits of this type of hybridization in island model were reported only recently~\cite{izzo2012},~\cite{thiago2014}.
%best metaheuristics to interplay together in obtaining the final solution.
%Fine-tuning parameters of the island model for given problem is cumbersome, thus a 
% TODO: Find the bibtex for the Thiago guy

In this work we focus on the hybridization in terms of distinct metaheuristics, and provide the performance analysis with respect to the migration operator.
We propose a parametrization of migration rate and topology expressed in terms of migration probability, and investigate the interplay between selected hyper-parameters of the island model.
We show that CRAZY STUFF HAPPENS BUT IT IS GOOD.
%Structure of this paper is as follows

\textbf{Add Motivation}

\section{Experimental setup}
This section covers the details of our experimental setup, i.e. used metaheuristics, problems, experimental variables and performance evaluation.

\subsection{Metaheuristics}

The topic of our study were three stochastic and population-based optimization metaheuristics.
In this section we briefly explain each of them.%TODO: Fix this sentence

\subsubsection{Differential Evolution (DE)}
The Differential Evolution algorithm proposed by Storn and Price~\cite{storn1997differential} is a simple yet robust iterative search process.
The DE algorithm creates new individual by selecting a triple $\vec{x_1}, \vec{x_2}$ and $\vec{x_3}$ at random without replacement from the current population $P$, and computing:
%$$\vec{x_k} = F \cdot (\vec{x_1} - \vec{ + F \cdot (\vec{x_2} - \vec{x_3}),$$
$$\vec{x}_{i*}^{(t+1)} = \vec{x}_1^{(t)} + F \cdot (\vec{x}_2^{(t)} - \vec{x}_3^{(t)}),$$
with scaling factor $F=0.5$.
Solution $x_i^*$ replaces $x_i$ if $f(x_i^*) < f(x_i)$ (assuming minimization).

\subsubsection{Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES)}

The CMA-ES proposed by Hansen et al.~\cite{hansen2001completely},~\cite{hansen2003reducing} optimizes the objective function by sampling $\lambda$ solutions ($i = 1,\ldots,\lambda)$ from the multivariate normal distribution:
$$x_i^{(t + 1)} = \textbf{m}^{(t)} + \sigma^{(t)} \cdot {\cal N}(0, {\cal C}^{(t)}),$$

where $\textbf{m}^{(t)}$, ${\cal C}^{(t)}$ and $\sigma^{(t)}$ denote mean, covariance matrix and step-size of the distribution at time $t$.
Out of $\lambda$ newly sampled and evaluated individuals, $\mu$ best are chosen and used for updating the parameters of the distribution.

\subsubsection{Particle Swarm Optimization (PSO)}

PSO is a bio-inspired stochastic search technique.
$$
    x_i^{(t+1)} = x_i^{(t)} + v_i^{(t)},
$$
where the velocity $v_i^{(t)}$ is updated as
$$
v_i^{(t+1)} = w(v_i^{(t)}) + c_1 \cdot \theta\cdot(pbest(x_i^{(t)}) - x_i^{(t)}) + c_2 \cdot \theta\cdot(gbest(x_i^{(t)}) - x_i^{(t)}).
$$

\subsection{Problems}

We test each hyper-parametrization setup on selected rotated variants of \emph{Sphere}, \emph{Discus}, \emph{Rosenbrock}, \emph{Schaffer F7}, \emph{Rastrigin} and \emph{Schwefel} functions, as proposed in the CEC 2013 benchmark suite (functions $f_1$, $f_4$, $f_6$, $f_7$, $f_{12}$ and $f_{15}$ respectively)~\cite{liang2013problem}.

\subsection{Heterogeneous Migration Model (HMM)}
Heterogeneous migration model described in this paper is in fact an island model, in which the concepts of \emph{topology} and \emph{migration rate} are unified in a single parametrization called \emph{migration probability}.
Migration rate $r$ and migration probability $p$ are two ways of parametrizing a single concept, i.e. the rate of information passing from one population to another (CITE TO Cantu PAZ about migration).
Although migration rate is a more popular parameterization, in this paper we will use the migration probability, as it allows for expressing ``fractional'' migration rates, e.g. for $p=0.75$, we can expect on average to migrate in $3$ out of $4$ possible synchronization points.
, and can be seen as reciprocals ($ 1/r \simeq p$).
%We allow HMM to have a migration rate dependent on the pair of islands exchanging information.
In HMM, probability of migration is dependent on the pair of populations exchanging the information.
This information can be stored as the weights in the directed acyclic graph of the model, thus replacing the global migration rate with a custom parameter determining the migration rate between any two populations.
For $n$ islands, the number of parameters modeling the probability of migration is equal to $n^2 - n$.
In reference to the island model, when $p_1=p_2=\ldots=p_{n^2-n}=c$ we obtain a fully--connected topology with the uniform ``migration rate'' equal to $\dfrac{1}{c}$ .
By setting some $m$ of those $n^2-n$ parameters to zero: $p_{k_1} = p_{k_2} = \ldots = p_{k_m} = 0$, we can obtain any other possible topology.
%Thus, HMM allows for more customization of the migration operator.

Because tuning such large space of parameters is very costly, we propose a reduced parametrization of HMM.
We assume that the probability of migration (directed edge) between given two \textbf{metaheuristics} is the same across the whole model, i.e. any two populations $P_i, P_j$ optimized by algorithms $H_k, H_l$ respectively, have the same probability of migration from $P_i$ to $P_j$ (see Figure~\ref{fig:2-4setup}).
This depends the number of parameters only on the number of distinct metaheuristics.

%We propose a setup with two metaheuristics running in parallel and allow for migration of the best solutions every generation with some probability $p_k$.
%We execute our two--island model synchronously, and allow for migration steps (every single generation).

\begin{figure}[htp]
\centering
\subfigure[$2$--heuristic, $4$--island setup ($4$ parameters)]{
    \label{fig:2-4setup}
    \begin{tikzpicture}[scale=2.5]
    \tikzstyle{vertex}=[draw,text=black,auto=left,circle,fill=white,minimum size=15pt]
    \tikzstyle{vertexred}=[draw,text=black,auto=left,color=red,circle,fill=white,minimum size=15pt]
    \tikzstyle{weight} = [fill=white]
    \tikzstyle{edge} = [draw,thick,->]
    \tikzstyle{edge2} = [draw,thick,-]

    \node[vertex] (a) at (1, 1) {$H_2$};
    \node[vertex] (b) at (0, 0) {$H_2$};
    \node[vertex] (c) at (1, 0) {$H_1$};
    \node[vertex] (d) at (0, 1) {$H_1$};

    \path[edge2] (a) to[near start] node[weight]{$p_1$} (b) ;
    \path[edge2] (c) to[near end] node[weight]{$p_4$} (d) ;
    %\path[edge] (b) to[bend right=18] node[weight]{$p_1$} (a);
    \path[edge] (a) to[bend right=18] node[weight]{$p_2$} (c) ;
    \path[edge] (c) to[bend right=18] node[weight]{$p_3$} (a);
    \path[edge] (b) to[bend right=18] node[weight]{$p_2$} (c);
    \path[edge] (c) to[bend right=18] node[weight]{$p_3$} (b) ;

    \path[edge] (a) to[bend right=18] node[weight]{$p_2$} (d) ;
    \path[edge] (d) to[bend right=18] node[weight]{$p_3$} (a);

    \path[edge] (b) to[bend right=18] node[weight]{$p_2$} (d);
    \path[edge] (d) to[bend right=18] node[weight]{$p_3$} (b) ;
    \end{tikzpicture}
}\quad
\subfigure[$3$--heuristic, $3$--island setup ($6$ parameters)]{

    \begin{tikzpicture}[scale=2.5]
    \tikzstyle{vertex}=[draw,text=black,auto=left,circle,fill=white,minimum size=15pt]
    \tikzstyle{vertexred}=[draw,text=black,auto=left,color=red,circle,fill=white,minimum size=15pt]
    \tikzstyle{weight} = [fill=white]
    \tikzstyle{edge} = [draw,thick,->]
    \tikzstyle{edge2} = [draw,thick,-]

    \node[vertex] (a) at (0.3, 1) {$H_1$};
    \node[vertex] (b) at (0, 0) {$H_2$};
    \node[vertex] (c) at (1, 0.3) {$H_3$};

    \path[edge] (a) to[bend right=18] node[weight]{$p_3$} (c) ;
    \path[edge] (c) to[bend right=18] node[weight]{$p_4$} (a);

    \path[edge] (c) to[bend right=18] node[weight]{$p_5$} (b) ;
    \path[edge] (b) to[bend right=18] node[weight]{$p_6$} (c);

    \path[edge] (a) to[bend right=18] node[weight]{$p_1$} (b) ;
    \path[edge] (b) to[bend right=18] node[weight]{$p_2$} (a);

    \end{tikzpicture}
}
\caption{Examples of HMM setups}
\label{fig:setups}
\end{figure}

In our definition we use the following:
\begin{enumerate}
    \item $f$ -- optimized problem
    \item $\mathbb{P}$ -- A list of populations% instances with random initializations, in this paper we consider
        %$2 \leq |\mathbb{P}| \leq 8$
    \item $\mathbb{H}$ -- A list of metaheuristics, in this paper we consider
        $$\mbox{Domain}(\mathbb{H}) = \{\mbox{DE}, \mbox{PSO}, \mbox{CMA-ES}\}$$
    \item $h: \mathbb{P} \rightarrow \mathbb{H}$ -- an assignment of a metaheuristic to a population (many populations can be evolved by the same type of heuristic)
    \item $M: \mathbb{H} \times \mathbb{H} \rightarrow [0, 1]$ -- mapping from a pair of metaheuristics to the probability of migration% and a real value ranging from 0 and 1, modeling the probability of migration

\end{enumerate}

\begin{algorithm}
\caption{Main optimization procedure}
\label{alg:main_loop}
\begin{algorithmic}[1]
    \Procedure{Optimize}{$f, \mathbb{P}, h, M$}%\Comment{Single solution from population $P_1$ to $P_2$}
    \For{$P \in \mathbb{P}$}
        \State $P \gets \Call{Initialize}{f}$
    \EndFor
    \Do
        \For{$P \in \mathbb{P}$}
            \State $P \gets \Call{Evolve}{P, f, h(P)}$\Comment{Evolve population $P$, on a function $f$, using algorithm $h(P)$}
        \EndFor
        \For{$P_i, P_j \in \mathbb{P} \times \mathbb{P}, i\neq j$}\Comment{For each possible pair of populations}
            \State $x \sim U(0,1)$\Comment{Sample uniformly from $(0, 1)$}
            \If{$x < M(h(P_i), h(P_j))$}\Comment{Migrate if passed the probability threshold}\\
                \State $\Call{Migrate}{P_i, P_j}$
            \EndIf
        \EndFor
        \DoWhile{not $\Call{MeetsStopCriteria}{\mathbb{P}}$}
        \State $f_0, FE \gets \Call{GetResults}{\mathbb{P}}$\Comment{The best solution $f'$ and number of function evaluations $B'$}
\EndProcedure \mbox{ and }\Return $f_0, FE$
\end{algorithmic}
\end{algorithm}
The migration algorithm which we use is based on the ``best--replace--worst'' idea (CITE HERE), i.e. the incoming solution, if better than the worst solution in the population, will take its place (see Algorithm~\ref{alg:migr_1}).
\begin{algorithm}
\caption{Migration algorithm}
\label{alg:migr_1}
\begin{algorithmic}[1]
\Procedure{Migrate}{$P_1,P_2$}\Comment{Migrate from $P_1$ to $P_2$}
    \If{$\Call{Best}{P_1} < \Call{Worst}{P_2}$}\Comment{Assuming minimization}
        \State $\Call{Worst}{P_2} \gets \Call{Best}{P_1}$
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

    %Main drawback of this idea is the possibility of duplicating solutions in the population.
    %As long as $\Call{Worst}{P} > m$, the solution $m$ will be injected to population $P$.
    %This creates a problem as it effectively diminishes the population size -- intuitively, there is little gain in having duplicated information occupying a slot in the population.
    %We propose two variants of the algorithm~\ref{alg:migr_1} -- unique information filtering (Algorithm~\ref{alg:migr_3}) and fixed--index replacement (Algorithm~\ref{alg:migr_2}).
    %First variant adds a condition to the migration, such that the migrant coming from the population $P_1$ has to be unique within the receiving population $P_2$.
    %Verifying this condition introduces additional computational cost to the process.
    %Second variant avoids the problem entirely by assigning a fixed position in the population for all migrant solutions.
    %In such case the duplication is not possible, as the incoming duplicate $m$ would at most replace its previous instance at the fixed position.
    %This prevents only the ``direct'' duplication through migration process, as the population $P_2$ is free to create duplicates of $m$ in the meantime.
    %For this purpose we define a function $\Call{M-Idx}{P}$, which returns a reference to the ``migration'' slot in the population $P$.

\subsection{Methodology}

Since we treat each of the islands as a parallel computational unit, we assign a fixed budget of $20,000$ function evaluations ($FE_{\mbox{max}}$) for each of them.
We stop the execution if any of the following conditions are met:
\begin{enumerate}
    \item A solution within $10^{-8}$ from the global optimum is found
    \item The function evaluation budget is exhausted
    \item Each population converges to a non-optimal point, i.e.\ the difference between the best and worst solution is smaller than $10^{-20}$
\end{enumerate}

We repeat the experiment $1,000$ times, obtaining a pool of independent runs, each of those could either be successful (condition 1 above) or unsuccessful (condition 2 or 3).
Each run can either a) find an optimal solution (successful run), b) exhaust the budget before arriving at a solution or c) converge to a local optimum (unsuccessful runs).

\subsection{Uniform migration experiment}
This section presents the core idea and methodology used in this paper, as well as serve as an introduction to our subsequent exhaustive experiment.
In this and the following experiments we will observe the behaviour of metaheuristics with respect to migration operator in terms of running time distributions and expected running time (expected number of function evaluations to global minima convergence).

Let consider a two--island setup, where each population is evolved using two distinguished instaces of Differential Evolution algorithm running synchronously (denoted as DE$_1$ and DE$_2$).
To get a better image of the effect of migration we will not actively observe the island running algorithm DE$_1$, i.e. the stopping criteria and the number of function evaluations are considered and measured only locally for DE$_2$.
We will iterate over the probability of migration from algorithm DE$_1$ to DE$_2$ (parameter $p_1$), and consider two cases for the opposite (parameter $p_2$): 
\begin{enumerate}
    \item $p_2$ is always equal to 0 -- island DE$_2$ does not send any solutions to DE$_1$
    \item $p_2$ is equal to $p_1$ -- island DE$_2$ will send solutions to DE$_1$ with the same probability
\end{enumerate}
Figure~\ref{graph:twoisl} presents the experimental setup on a diagram.
\begin{figure}[ht!]
    \begin{tikzpicture}[scale=2.5]
    \tikzstyle{vertex}=[draw,text=black,auto=left,circle,fill=white,minimum size=24pt]
    \tikzstyle{vertexred}=[draw,text=black,auto=left,color=red,circle,fill=white,minimum size=15pt]
    \tikzstyle{weight} = [fill=white]
    \tikzstyle{edge} = [draw,thick,->]
    \tikzstyle{edge2} = [draw,thick,-]

    \node[vertex, dashed] (a) at (0, 0) {DE$_1$};
    \node[vertex] (b) at (1, 0) {DE$_{2}$};


    \node[text width=4cm] at (2.1, 0.0) {%$H = \{\mbox{DE}, \mbox{CMAES}\}$\\
        $p_1 = \{0.0, 0.1, \ldots , 0.9, 1.0\}$\\
        $p_2 = \{0.0, p_1\}$};

    \path[edge] (a) to[bend left=25] node[weight]{$p_1$} (b) ;
    \path[edge] (b) to[bend left=25] node[weight]{$p_2$} (a);

    \end{tikzpicture}
    \label{graph:twoisl}
    \caption{Two--island setup with both algorithms running synchronously and migrating with probabilities $p_1$ and $p_2$. We observe only the results of DE$_2$.}

\end{figure}
Figure~\ref{fig:single_jde_jde_9} presents the distribution of $1,000$ running times approximated by a kernel density estimation using Gaussian kernel.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\columnwidth]{figures/single_jde_jde_9.png}
  \label{fig:single_jde_jde_9}
 \caption{Running time behaviour of homogeneous 2--island DE/DE setup on problem $f_{12}$.}
\end{figure}

For $p_2=0$ (DE$_2$ does not send any feedback to DE$_1$) the migration has a positive effect -- by allowing for migration with probability $p_1 = 0.1$ the mean shifts from $3,360$ function evaluations to $3,100$.
It is worth noting that the difference between cases $p_1=0.1$ and $p_1 > 0.1$ are considerably less significant when compared to the ``feedback'' scenario, where $p_2=p_1$.
For $p_1=p_2=0.1$ the mean is shifted to $2,900$ and continues up until $p_1=p_2=1.0$ with mean being equal to a little over $1,560$.
This proves that migration, even when observed strictly from the perspective of the ``receiver'', is not a one--directional process.
Sending current good solutions to the neighboring populations can result in future migration of even superior solutions.

Let us modify our experiment and replace the ``sending'' island DE$_1$ with an instance of CMAES.
Both algorithms are run synchronously, and as previously we will observe only the distribution of convergence times of population evolved by D$_2$ (parameters $p_1$ and $p_2$ are modified as before).

Before we continue, let us look at the distribution of convergence times of single and independent runs of CMAES and DE on problem $f_9$ (see Figure~\ref{fig:distr_cmaes_jde_9}).
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{figures/single_distr_cmaes_jde_9.png}
    \label{fig:distr_cmaes_jde_9}
    \caption{Distribution of running times for algorithms CMA-ES and DE on problem $f_9$.}
\end{figure}
It is quite clear that on this particular problem, CMAES converges to global minimum much faster than DE.
Not surprisingly, when we allow for injection from CMAES to DE (see Figure~\ref{fig:single_cmaes_jde_9}), i.e. $p_1=0.1$ and $p_2=0$, we notice a significant improvement in the running time distribution (mean shift from $3,360$ to $2,120$).
\begin{figure}[ht]
  \centering
  \includegraphics[width=\columnwidth]{figures/single_cmaes_jde_9.png}
  \label{fig:single_jde_jde_9}
 \caption{Running time behaviour of homogeneous 2--island DE/DE setup on problem $f_{12}$.}
\end{figure}
As before, the probability of migration in one--directional setup starts to reach its limit as soon as $p_1 > 0.1$.
When $p_2 = p_1$ however, we notice a gradual improvement of running times distribution up to $p_1=p_2=1.0$ (mean shift to $1,520$).
It is worth noting that even though CMAES was previously established as ``faster'' algorithm for this problem, the feedback from the ``slower'' DE$_2$ was still beneficial.

This result motivates us to consider not only ``fully--connected'' topologies -- even in cases where one might think that better-suited algorithms might not benefit from the solutions from ``slower'' algorithms -- but also explore the space of asymmetrical migration parameters, i.e. $p_1 \neq p_2$.

\subsection{Varied migration experiment}

This section covers the methodology and the results of our main experiment, which extends the previous setup by allowing parameters $p_1$ and $p_2$ to have distinct values from $\{0.0, 0.1, \ldots, 1.0\}$.
Additionally, we will monitor the stopping criteria and number of function evaluations at the scope of the whole archipelago.
%One might be tempted to continue with the ``local'' analysis for non--uniform migration probabilities and then extend this reasoning to a global scale -- if both islands are synchronized, and we terminate the execution as soon as \emph{any} algorithm finds the solution within global optimum, it is then an equivalent process to sampling a run from each individual island distributions $A$, $B$, and assume the number of total function evaluations (at the archipelago scope) as: FE$ = \mbox{min}(\Call{Sample}{A}, \Call{Sample}{B})$.
%We decide against it as the process get more complicated on difficult multi--modal and introduce the restarts mechanism.
%For that reason, we will monitor the stopping criteria and the running time distribution across the whole setup, and use the 
Since we will extend our test to the whole range of problems, some of which being multi--modal, we expect the stopping criteria to detect the condition of convergence to suboptimal solution.
In such a case in a real--world scenario, a practitioner would proceed the optimization by restarting the algorithm as long as the budget is not exhausted.
In order to compensate for this, we employ a boostrap resampling technique as presented in Algorithm~\ref{alg:resample}.

%In real budget--constrained scenario, a practitioner would try to detect the third case by analyzing the convergence rate and diversity in the population and restart given algorithm with random initial conditions as long as the budget permits it.
%We allow for such operation, by employing a resampling technique, using which we construct a ``trial'', i.e., a sequence of runs among which the solution is found, or the budget is definitely exhaused (see Alg.~\ref{alg:resample}).

\begin{algorithm}
\caption{Resampling pool of runs $R$ within budget $B$}
\label{alg:resample}
\begin{algorithmic}[1]
    \Function{Resample}{$R, B$}%\Comment{Pool $M$ and budget $B$}
    \State $B' \gets 0$\Comment{Trial budget}
    \State $f' \gets \infty$\Comment{Trial objective function value}
    \Do
        \State $f, n \gets \Call{SampleRun}{R}$%\Comment{Objective value $f$ and number of function evaluations $n$}
        \State $B' \gets \mbox{max}(B' + n, B)$
        \State $f' \gets \mbox{min}(f', f)$
        \DoWhile{$B' < B \And (f' - f_{min}) > 10^{-8}$}\Comment{Stop building a trial after budget is exhausted or found optimal solution}
    \EndFunction \mbox{ and }\Return $f', B'$
\end{algorithmic}
\end{algorithm}

To assess the performance of our setup we use the measure of expected number of function evaluations as proposed by Auger and Hansen in~\cite{Hansen05performanceevaluation}:

\begin{equation}
    \widehat{\mathbb{E}(T_H)} = \left(\dfrac{1 - \widehat{p_s}}{\widehat{p_s}}\right)FE_{\mbox{max}} + \widehat{\mathbb{E}(T_H^s)},
\end{equation}
where $FE_{\mbox{max}}$ is the allocated budget, $\widehat{p^s}$ is the estimated probability of successful run and $\widehat{\mathbb{E}(T_H^s)}$ is the estimated number of function evaluations for successful runs of hybrid metaheuristics setup $H$.


\subsection{Taxonomy of expected running time patterns}

When analyzing the results of our experiment we can clearly see a distinction between the unimodal and multi--modal problems.
Figure~\ref{fig:jde_pso_6} presents the most typical running time patterns.% for problems $f_1$, $f_4$, $f_6$, $f_7$ and $f_9$.
\begin{figure}[ht!]
\centering
\subfigure[t][foo]{
    \label{tax:tax1}
    \includegraphics[width=0.45\columnwidth]{figures/taxonomy_cmaes_cmaes_cec_4.png}
}\quad
\subfigure[t][bar]{
    \label{tax:tax2}
    \includegraphics[width=0.45\columnwidth]{figures/taxonomy_jde_jde_cec_4.png}
}\quad
\subfigure[t][bar]{
    \label{tax:tax3}
    \includegraphics[width=0.45\columnwidth]{figures/taxonomy_cmaes_jde_cec_1.png}
}\quad
\subfigure[t][bar]{
    \label{tax:tax4}
    \includegraphics[width=0.45\columnwidth]{figures/taxonomy_cmaes_jde_cec_6.png}
}
\caption{fff f} %Expected running time behaviour of 2--m/2--i setup on $f_{4}$ problem.}
\end{figure}

\begin{table*}
    \begin{tabular}{c|c|c|c|c|c|c|}
        %\cline{2-3}
        %\hline
        & \multicolumn{6}{c}{$H_1$/$H_2$ on pattern~\ref{tax:tax2}} \\\hline
        Problems & CMAES/CMAES        & DE/DE              & PSO/PSO               & CMAES/DE            & DE/PSO              & CMAES/PSO \\\hline
        $f_1$    & $p=1256$, $k=1.12$ & $p=1293$, $k=1.51$ & $p=3797$,  $k=3.17$   & $p=1243$, $k=1.07$  & $p=2448$, $k=$  & $p=2292$, $k=$\\
        $f_4$    & $p=2048$, $k=1.37$ & $p=2414$, $k=1.68$ & -                     & $p=2056$, $k=1.08$  & -               & -             \\
        $f_6$    & $p=1490$, $k=1.17$ & $p=1583$, $k=1.78$ & $p=9378$,  $k=3.10$   & $p=1368$, $k=1.27$  & $p=3431$, $k=$  & $p=2754$, $k=$\\
        $f_7$    & $p=3958$, $k=2.11$ & $p=4114$, $k=1.58$ & $p=10374$, $k=2.94$   & $p=3354$, $k=1.45$  & $p=7273$, $k=$  & $p=6454$, $k=$\\
        $f_9$    & $p=3233$, $k=1.67$ & $p=3486$, $k=1.78$ & $p=11552$, $k=2.78$   & $p=3054$, $k=1.33$  & $p=6823$, $k=$  & $p=6043$, $k=$\\
        $f_{12}$ &                    & -                  & $p=13396$, $k=1.42$   & -                   & -               & $p=5972$, $k=$\\
        $f_{13}$ &                    & $p=2784$, $k=1.42$ & $p=6734$,  $k=2.42$   & $p=2681$, $k=1.33$  & $p=4947$, $k=$  & $p=4727$, $k=$\\
        $f_{15}$ &                    & $p=2784$, $k=1.42$ & $p=6734$,  $k=2.42$   & $p=2681$, $k=1.33$  & $p=4947$, $k=$  & $p=4727$, $k=$\\
        $f_{20}$ &                    & $p=2784$, $k=1.42$ & $p=6734$,  $k=2.42$   & $p=2681$, $k=1.33$  & $p=4947$, $k=$  & $p=4727$, $k=$\\
        %\hline
    \end{tabular}
\end{table*}

\begin{table}
    \begin{tabular}{c|c|c}
        %\cline{2-3}
        %\hline
        & \multicolumn{2}{c}{$H_1$/$H_2$ on pattern~\ref{tax:tax1}} \\\hline
        Problems & CMAES/CMAES         & JDE/PSO \\\hline
        $f_1$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_4$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_6$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_7$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_9$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        %\hline
    \end{tabular}
\end{table}

\begin{table}
    \begin{tabular}{c|c|c}
        %\cline{2-3}
        %\hline
        & \multicolumn{2}{c}{$H_1$/$H_2$ on pattern~\ref{tax:tax1}} \\\hline
        Problems & CMAES/CMAES         & JDE/PSO \\\hline
        $f_1$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_4$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_6$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_7$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        $f_9$    & $p=12344$, $k=2.44$ & $p=12344$, $k=2.44$ \\
        %\hline
    \end{tabular}
\end{table}

\begin{figure*}[htp]
  \centering
  \subfigure[Correlation between the migration probabilities and expected runnig time.]{
  \includegraphics[width=\columnwidth]{figures/jde_pso_cec_6_0.png}
  %\includegraphics[scale=0.38]{image1}
  }\quad
  \subfigure[Distribution of $1,000$ runs (left) and $10,000$ resampled trials for $P(DE\rightarrow DE)=0.0,  0.2$ and $1.0$.]{
  \includegraphics[width=\columnwidth]{figures/hist3_jde_pso_cec_6.png}
  }
  \label{fig:jde_pso_6}
 \caption{Running time behaviour of homogeneous 2--island DE/DE setup on problem $f_{12}$.}
\end{figure*}

In this section we provide a qualitative analysis of migration effects on the performance of our two-island model.
Due to space constraints we will restrict ourselves to reporting several most interesting cases.

\begin{figure*}[htp]
  \centering
  \subfigure[Correlation between the migration probabilities and expected runnig time.]{
  \includegraphics[width=\columnwidth]{figures/jde_jde_cec_12_0.png}
  }\quad
  \subfigure[Distribution of $1,000$ runs (left) and $10,000$ resampled trials for $P(DE\rightarrow DE)=0.0,  0.2$ and $1.0$.]{
  \includegraphics[width=\columnwidth]{figures/hist2_jde_jde_cec_12.png}
  }
 \caption{Running time behaviour of homogeneous 2--island DE/DE setup on problem $f_{12}$.}
\end{figure*}


In all of the homogeneous cases, we observe a symmetrical relation between the $p_1$ and $p_2$, although not always the identical one.
In this section we take a closer look at one of the expected running time patterns and provide insight on the effect of migration probability.
Let us look at the 2--metaheuristic, 2--island setup, where each island was assigned the same copy of the search algorithm.

%\begin{figure}[ht!]
%\centering
%\includegraphics[width=\columnwidth]{figures/jde_jde_cec_12_0.png}
%\caption{Expected running time behaviour of 2--m/2--i setup on $f_{12}$ problem.}
%\end{figure}


%\input{table.tex}

%\begin{figure}[ht!]
%\centering
%\includegraphics[width=\columnwidth]{figures/hist2_jde_jde_cec_12.png}
%\caption{}
%\end{figure}

\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{bib}

\end{document}
